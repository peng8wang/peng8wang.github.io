<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="lai.css" type="text/css" />
<title>Publications</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Peng Wang</div>
<div class="menu-item"><a href="index.html"><b>Home</b></a></div>
<div class="menu-item"><a href="publication.html" class="current"><b>Publications</b></a></div>
<div class="menu-item"><a href="position.html"><b>Openings</b></a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
<div id="subtitle">
</div>
</div>
<h2>Preprints (&lsquo;&lsquo;*&rsquo;&rsquo; denotes equal contribution, &lsquo;&lsquo;\(\dagger\)&rsquo;&rsquo; denotes corresponding author.)</h2>
<ul>
<li><p>(<font color=red size=+0.5><b>\(\alpha\)-\(\beta\) order</b></font>) Po Chen, Rujun Jiang, <b>Peng Wang</b>\(^\dagger\). A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization, 2025. [<a href="https://arxiv.org/pdf/2506.20344" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>(<font color=red size=+0.5><b>\(\alpha\)-\(\beta\) order</b></font>) Laura Balzano\(^\dagger\), Tianjiao Ding\(^\dagger\), Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, <b>Peng Wang</b>\(^\dagger\), Zhangyang Wang, Can Yaras. An Overview of Low-Rank Structures in the Training and Adaptation of Large Models. Under review in <b><i>IEEE Signal Processing Magazine</i></b>, 2025. [<a href="https://arxiv.org/pdf/2503.19859" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>(<font color=red size=+0.5><b>\(\alpha\)-\(\beta\) order</b></font>) Po Chen, Rujun Jiang, <b>Peng Wang</b>\(^\dagger\). Error Bound Analysis for the Regularized Loss of Deep Linear Neural Networks, 2025. [<a href="https://arxiv.org/pdf/2502.11152" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Xiao Li*, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu. Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. <b>Minor revision</b> in <b><i>Journal of Machine Learning Research</i></b>, 2024. [<a href="https://arxiv.org/pdf/2311.02960.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*\(^\dagger\), Huijie Zhang*, Zekai Zhang, Siyi Chen, Yi Ma, Qing Qu. Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering, 2024. Under review in <b><i>Journal of Machine Learning Research</i></b>, 2025. [<a href="https://arxiv.org/pdf/2409.02426" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
<ul>
<li><p><b>NeurIPS 2024 M3L Workshop</b> &amp; <b>ICLR 2025 DeLTa Workshop</b>.
</p>
</li></ul>
</li>
<li><p>Huijie Zhang, Zijian Huang, Siyi Chen, Jinfan Zhou, Zekai Zhang, Peng Wang, Qing Qu. Understanding Generalization in Diffusion Models via Probability Flow Distance. Under review in <b>NeurIPS 2025</b>. [<a href="https://arxiv.org/pdf/2505.20123" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Xiao Li*, Zekai Zhang*, Xiang Li, Siyi Chen, Zhihui Zhu, <b>Peng Wang</b>\(^\dagger\), Qing Qu. Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling, 2025. Under review in <b>NeurIPS 2025</b>. [<a href="https://arxiv.org/pdf/2502.05743" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Alec S Xu, Can Yaras, <b>Peng Wang</b>, Qing Qu. Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data, 2025. Under review in <b>SIAM Journal on Mathematics of Data Science</b>. [<a href="https://arxiv.org/pdf/2501.02364" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Wei Hu, Zhihui Zhu, Laura Balzano, Qing Qu. The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks, 2023. To be submitted. [<a href="https://arxiv.org/pdf/2306.01154.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Taoli Zheng, <b>Peng Wang</b>, Anthony Man-Cho So. A Linearly Convergent Algorithm for Rotationally Invariant L1-Norm Principal Component Analysis, 2022. [<a href="https://arxiv.org/pdf/2210.05066.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]


</p>
</li>
</ul>
<h2>Journal Papers </h2>
<ul>
<li><p><b>Peng Wang</b>, Rujun Jiang, Qingyuan Kong, Laura Balzano. A Proximal Difference-of-Convex Algorithm for Sample Average Approximation of Chance Constrained Programming. Accepted for publication in <b><i>INFORMS Journal on Computing</i></b>, 2025. [<a href="https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2024.0648" target=&ldquo;blank&rdquo;><b>paper</b></a>, <a href="https://github.com/peng8wang/2024.0648" target=&ldquo;blank&rdquo;><b>code</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So. Linear Convergence of Proximal Alternating Minimization Method with Extrapolation for L1-Norm Principal Component Analysis. <b><i>SIAM Journal on Optimization</i></b> (2023) 33(2):684-712. [<a href="https://arxiv.org/pdf/2107.07107.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Zirui Zhou, Anthony Man-Cho So. Non-Convex Exact Community Recovery in Stochastic Block Model. <b><i>Mathematical Programming, Series A</i></b> (2022) 195(1-2):793-829. [<a href="https://arxiv.org/pdf/2006.15843v4.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 

</p>
</li>
</ul>
<h2>Conference Papers</h2>
<ul>
<li><p><b>Peng Wang</b>, Yifu Lu, Yaodong Yu, Druv Pai, Qing Qu, Yi Ma. Attention-Only Transformers via Unrolled Subspace Denoising. <b><i>ICML 2025</i></b>. [<a href="https://arxiv.org/pdf/2506.03790" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Can Yaras*, Siyi Chen*, Peng Wang, Qing Qu. Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning. <b><i>CAPL 2025</i></b>. [<a href="https://arxiv.org/pdf/2412.07909" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Siyi Chen*, Huijie Zhang*, Minzhe Guo, Yifu Lu, <b>Peng Wang</b>, Qing Qu. Exploring Low-Dimensional Subspaces in Diffusion Models for Controllable Image Editing. <b><i>NeurIPS 2024</i></b>. [<a href="https://arxiv.org/pdf/2409.02374" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Druv Pai, Yaodong Yu, Zhihui Zhu, Qing Qu, Yi Ma. A Global Geometric Analysis of Maximal Coding Rate Reduction. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2406.01909" target=&ldquo;blank&rdquo;><b>paper</b></a>]  
</p>
</li>
<li><p>Can Yaras, <b>Peng Wang</b>, Laura Balzano, Qing Qu. Compressible Dynamics in Deep Overparameterized Low-Rank Learning &amp; Adaptation. <b><i>ICML 2024</i></b> (<font color=red size=+0.5><b>Oral, acceptance rate: 1.52%</b></font>). [<a href="http://arxiv.org/abs/2406.04112" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huikang Liu*, <b>Peng Wang</b>*, Longxiu Huang, Qing Qu, Laura Balzano. Matrix Completion with ReLU Sampling. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2406.05822" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Jiachen Jiang, Jinxin Zhou, <b>Peng Wang</b>, Qing Qu, Dustin Mixon, Chong You, Zhihui Zhu. Generalized Neural Collapse for a Large Number of Classes. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2310.05351.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, <b>Peng Wang</b>, Liyue Shen, and Qing Qu. The Emergence of Reproducibility and Consistency in
Diffusion Models. <b><i>ICML 2024</i></b>. [<a href="https://arxiv.org/pdf/2310.05264.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Wei Hu, Zhihui Zhu, Laura Balzano, Qing Qu. Invariant Low-Dimensional Subspaces in Gradient Descent for Learning Deep Matrix Factorizations. <b><i>NeurIPS M3L Workshop 2023</i></b>. [<a href="https://openreview.net/pdf?id=4pPnQqUMLS" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Jinxin Wang, Yuen-Man Pun, Xiaolu Wang, <b>Peng Wang</b>, Anthony Man-Cho So. Projected Tensor Power Method for Hypergraph Community Recovery. <b><i>ICML 2023</i></b>.  [<a href="https://openreview.net/pdf?id=CcDKqUR546" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Huikang Liu*, Can Yaras*, Laura Balzano, Qing Qu. Linear Convergence Analysis of Neural Collapse with Unconstrained Features. NeurIPS Workshop on Optimization for Machine Learning, <b><i>NeurIPS OPT Workshop 2022</i></b>. [<a href="https://openreview.net/pdf?id=WC9im-M_y5" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Can Yaras*, <b>Peng Wang</b>*, Zhihui Zhu, Laura Balzano, Qing Qu. Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold. <b><i>NeurIPS 2022</i></b>. [<a href="https://arxiv.org/pdf/2209.09211.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So, Laura Balzano. Convergence and Recovery Guarantees of the K-Subspaces Method for Subspace Clustering. <b><i>ICML 2022</i></b>. [<a href="https://arxiv.org/pdf/2206.05553.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>] 
</p>
</li>
<li><p>Xiaolu Wang, <b>Peng Wang</b>, Anthony Man-Cho So. Exact Community Recovery over Signed Graphs. <b><i>AISTATS 2022</i></b>. [<a href="https://arxiv.org/pdf/2202.12255.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Zirui Zhou, Anthony Man-Cho So. Optimal Non-Convex Exact Recovery in Stochastic Block Model via Projected Power Method. <b><i>ICML 2021</i></b>. [<a href="https://arxiv.org/pdf/2106.05644.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>*, Zirui Zhou*, Anthony Man-Cho So. A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model. <b><i>ICML 2020</i></b>. [<a href="http://proceedings.mlr.press/v119/wang20ac/wang20ac.pdf" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p><b>Peng Wang</b>, Huikang Liu, Anthony Man-Cho So. Globally Convergent Accelerated Proximal Alternating Maximization Method for L1-Principal Component Analysis. <b><i>ICASSP 2019</i></b> (IEEE SPS Student Travel Award). [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8682499" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
<li><p>Huikang Liu, <b>Peng Wang</b>, Anthony Man-Cho So. Fast First-Order Methods for the Massive Robust Multicast Beamforming Problem with Interference Temperature Constraints. <b><i>ICASSP 2019</i></b>. [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683524" target=&ldquo;blank&rdquo;><b>paper</b></a>]
</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
